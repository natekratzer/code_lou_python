{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Did Louisville recover from recession more quickly than similar cities?\n",
    "\n",
    "First we'll import libraries. The altair package can be installed this way: conda install altair --channel conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'altair'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9d9fe3925a86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0maltair\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'notebook'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'altair'"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "alt.renderers.enable('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in an excel sheet of jobs related data for Louisville and for peer cities (source: Greater Louisville Project). You can use the Download Data button on this website to download a copy: http://greaterlouisvilleproject.org/deep-drivers-of-change/21st-century-jobs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df = pd.read_excel('GLP-Codebook.xlsx', 'Jobs County', index_col=None, na_values=['NA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a sql database that can be queried from in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df.to_sql(\"jobs_table\", sqlite3.connect(\"jobs.db\"), if_exists = \"replace\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostly pandas, some SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying back just the unemployment, median earnings, and personal income per capita data by year and city. Only for cities that are currently peers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"jobs.db\")\n",
    "jobs_df = pd.read_sql_query(\"SELECT year, city, unemployment, median_earnings, personal_income_per_cap FROM jobs_table WHERE current = 1\", con)\n",
    "jobs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making year into a datetime, subsetting to just Louisville, peers without Louisville, and finding mean of non-Louisville cities. Note: one nice thing about the groupby and mean code is that it doesn't change for multiple variables or require a loop. I changed the query above to include more than just 'unemployment' as shown in class, but didn't have to change the groupby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df['year'] = pd.to_datetime(jobs_df['year'], format = \"%Y\") #year column as datetime instead of character\n",
    "lou_df = jobs_df[(jobs_df.city == \"Louisville\")] # splitting the dataframe\n",
    "peer_df = jobs_df[(jobs_df.city != \"Louisville\")]\n",
    "mean_df = peer_df.groupby('year', as_index = False).mean() #groupby and aggregate\n",
    "mean_df['city'] = \"Peers\"\n",
    "df = mean_df.append(lou_df)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next code chunk loops over the three variables and graphs them. Jupyter notebook limits visuals to one chart per cell (if someone knows how to override this let me know), so I've saved the charts as separate .html files. They're saved in the working directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list = ['unemployment', 'median_earnings', 'personal_income_per_cap']\n",
    "for var in var_list:\n",
    "    chart = alt.Chart(df).mark_line().encode(\n",
    "    x='year',\n",
    "    y= var,\n",
    "    color='city') #group data and lines by this variable\n",
    "    chart.save(var+'_chart.html') #saves to working directory as 'unemployment_chart.html', etc. for each variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want the charts in the notebook though? One workaround is to write a function that we can then call in separate cells. This function is specific to our current dataframe - the only thing that changes is the name of the column. It looks a lot like the for loop, but instead of automatically looping through a list of variables, we have to pass the variable names to the function. One advantange though is in the possible extenions we could make in the future (e.g. a title for the graph or different axis labels could be an argument that gets passed to a function and so can vary graph by graph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph (var):\n",
    "    chart = alt.Chart(df).mark_line().encode(\n",
    "        x='year',\n",
    "        y= var,\n",
    "        color='city'\n",
    "    )\n",
    "    return chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Unemployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_graph('unemployment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_graph('median_earnings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_graph('personal_income_per_cap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mostly SQL, some pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try a more SQL based approach. We'll just worry about unemployment for now.\n",
    "\n",
    "First, pull down Louisville data and rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lou_df = pd.read_sql_query(\"\"\"\n",
    "SELECT year, unemployment as 'lou_unemployment'\n",
    "FROM jobs_table \n",
    "WHERE current = 1 AND city = 'Louisville'\n",
    "\"\"\", con)\n",
    "lou_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then pull down peer data, rename, and groupby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peer_df = pd.read_sql_query(\"\"\"\n",
    "SELECT year, avg(unemployment) as 'peer_unemployment'\n",
    "FROM jobs_table \n",
    "WHERE current = 1 AND city != 'Louisville'\n",
    "GROUP BY year\n",
    "\"\"\", con)\n",
    "peer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on how complex the work is you can save these new tables back into the same database for future queries. Probably not necessary in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peer_df.to_sql(\"peer_unemp_table\", sqlite3.connect(\"jobs.db\"), if_exists = \"replace\")\n",
    "lou_df.to_sql(\"lou_unemp_table\", sqlite3.connect(\"jobs.db\"), if_exists = \"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can join the two tables in SQL. Note that when dealing with multiple tables we can specify the field we're trying to pull with table_name.field_name (e.g. lou_unemp_table.year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(\"\"\"\n",
    "SELECT lou_unemp_table.year, lou_unemployment, peer_unemployment\n",
    "FROM lou_unemp_table\n",
    "JOIN peer_unemp_table\n",
    "ON lou_unemp_table.year = peer_unemp_table.year\n",
    "\"\"\", con)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for reference because the pandas code above appends rows to the dataframe, it is also possible to execute SQL-style joins in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(lou_df, peer_df, how = 'outer', left_on = ['year'], right_on = ['year'])\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
